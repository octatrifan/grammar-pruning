{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "vM9f2Krba6_5",
    "outputId": "2385b8a3-c1a1-464c-dee3-6a23bd059fdc"
   },
   "source": [
    "import guidance\n",
    "from guidance import models, gen, one_or_more, select, zero_or_more, regex, optional, capture\n",
    "\n",
    "model_name = \"your_model_name_here\"\n",
    "\n",
    "model = models.LlamaCpp(f\"{model_name}.gguf\", n_gpu_layers=-1, n_ctx=2048)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SZwQ3fBfbHnF"
   },
   "source": [
    "instruction_generate_burger = \"\"\"You are a helpful assistant. You have to take as input a customer order and output a list of the corresponding objects. You should use only the following classes in Python:\n",
    "      class Topping:\n",
    "            def __init__(self, name: str, qualifier: Optional[str] = None, negation: Optional[bool] = False) -> None:\n",
    "            \n",
    "      class BurgerOrder:\n",
    "            def __init__(self, number: int = 1, size: Optional[str] = None, main_dish_type: Optional[str] = None, toppings: Optional[List[Topping]] = None) -> None\n",
    "       \n",
    "      class DrinkOrder:\n",
    "            def __init__(self, number: int = 1, drink_type: Optional[str] = None, size: Optional[str] = None) -> None :\n",
    "      \n",
    "      class SideOrder:\n",
    "            def __init__(self, number: int = 1, side_type: Optional[str] = None, size: Optional[str] = None) -> None :\n",
    "      \n",
    "      The output should be a list of those objects.\\n\n",
    "      Here's an example:\n",
    "      'input': 'i would like a vegan burger with lettuce tomatoes and onions and a large order of sweet potato fries',\n",
    "      'output': '[BurgerOrder(number=1, main_dish_type='vegan_burger', toppings=[Topping(name='lettuce'), Topping(name='tomato'), Topping(name='onion')]), SideOrder(number=1, side_type='french_fries', size='large')]\\n',\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.language import Language\n",
    "import os\n",
    "import re\n",
    "\n",
    "def parse_line(line):\n",
    "    parts = line.strip().split('\\t')\n",
    "    if len(parts) != 2:\n",
    "        return None, None\n",
    "    phrase, category = parts\n",
    "    return phrase, category.strip()\n",
    "\n",
    "def get_all_ngrams(tokens, max_n):\n",
    "    ngrams = set()\n",
    "    for n in range(1, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = \" \".join([token.text for token in tokens[i:i + n]])\n",
    "            ngrams.add(ngram)\n",
    "    return ngrams\n",
    "\n",
    "def init_pipeline(dataset=\"burger\"):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    ner_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"phrase_matcher_attr\": \"LOWER\"})\n",
    "    \n",
    "    def read_file_categories(food_type):\n",
    "        file_path = f\"FoodOrderingDataset/data/{food_type}/alias\"\n",
    "        text_files = [f for f in os.listdir(file_path) if f.endswith('.txt')]\n",
    "        patterns = []\n",
    "        for file in text_files:\n",
    "            path_to_file = os.path.join(file_path, file)\n",
    "            with open(path_to_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        phrase, category_info = parse_line(line)\n",
    "                        if phrase and category_info:\n",
    "                            schema = {\"pattern\": phrase, \"label\": category_info}\n",
    "                            patterns.append(schema)\n",
    "        return patterns\n",
    "\n",
    "    category_patterns = read_file_categories(dataset)\n",
    "    ner_ruler.add_patterns(category_patterns)\n",
    "\n",
    "    global DRINK_KEYWORDS, SIDE_KEYWORDS\n",
    "    DRINK_KEYWORDS = {pat[\"pattern\"].lower() for pat in category_patterns if pat[\"label\"].startswith(\"DRINK_TYPE\")}\n",
    "    SIDE_KEYWORDS = {pat[\"pattern\"].lower() for pat in category_patterns if pat[\"label\"].startswith(\"SIDE_TYPE\")}\n",
    "    \n",
    "    nlp.add_pipe(\"disambiguate_size\", after=\"ner\")\n",
    "    \n",
    "    return nlp\n",
    "\n",
    "@Language.component(\"disambiguate_size\")\n",
    "def disambiguate_size(doc):\n",
    "    new_ents = []\n",
    "    \n",
    "    max_drink = max((len(phrase.split()) for phrase in DRINK_KEYWORDS), default=1)\n",
    "    max_side = max((len(phrase.split()) for phrase in SIDE_KEYWORDS), default=1)\n",
    "    max_n = max(max_drink, max_side)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if \"SIZE\" in ent.label_:\n",
    "            start = max(0, ent.start)\n",
    "            end = min(len(doc), ent.end + 0)\n",
    "            context_tokens = []\n",
    "            while end <= len(doc):\n",
    "                context_tokens = [token for token in doc[start:end]]\n",
    "                context_ngrams = get_all_ngrams(context_tokens, max_n)\n",
    "                if context_ngrams & DRINK_KEYWORDS:\n",
    "                    new_ent = spacy.tokens.Span(doc, ent.start, ent.end, label=\"DRINK_\" + ent.label_)\n",
    "                    new_ents.append(new_ent)\n",
    "                    break\n",
    "                elif context_ngrams & SIDE_KEYWORDS:\n",
    "                    new_ent = spacy.tokens.Span(doc, ent.start, ent.end, label=\"SIDE_\" + ent.label_)\n",
    "                    new_ents.append(new_ent)\n",
    "                    break\n",
    "                else:\n",
    "                    if end == len(doc) and start == ent.start:\n",
    "                        start -= 1\n",
    "                    else:\n",
    "                        end += 1\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = tuple(new_ents)\n",
    "    return doc\n",
    "\n",
    "def process_NER(input_order):\n",
    "    found_categories = []\n",
    "    doc = nlp(input_order)\n",
    "    for ent in doc.ents:\n",
    "        found_categories.append((ent.text, ent.label_))\n",
    "    return found_categories\n",
    "\n",
    "nlp = init_pipeline()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "p87BpXABbPpa",
    "outputId": "ad855072-afae-4f76-a5ba-c3860dd3bcc5"
   },
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "file_path = f'FoodOrderingDataset/output/Ablation_NER_Burger_Results_{model_name}.json'\n",
    "\n",
    "existing_data = []\n",
    "\n",
    "with open('FoodOrderingDataset/processed_data/burger_dataset_disambiguation.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "input_list = []\n",
    "for obj in data:\n",
    "    input_value = obj.get(\"input\", \"No input key found\")\n",
    "    output_value = obj.get(\"output_extract\", \"No output key found\")\n",
    "    output_generate = obj.get(\"output_generate\", \"No output key found\")\n",
    "    \n",
    "    used_items_value = process_NER(input_value)\n",
    "    used_items_value_decoupled = [f\"{ent_text} - {ent_label}\" for ent_text, ent_label in used_items_value]\n",
    "    used_items_str = ', '.join(used_items_value_decoupled).lower()\n",
    "\n",
    "    input_augmented_file = input_value + \"\\nItems Found: \" + used_items_str\n",
    "    input_list.append((input_value, input_augmented_file, output_generate, used_items_value, used_items_str))\n",
    "\n",
    "for i in range(len(input_list)):\n",
    "    if i > 130:\n",
    "        break\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Iteration \" + str(i))\n",
    "        \n",
    "    (initial_input, input_augmented, expected, used_items_value, used_items_str) = input_list[i]\n",
    "    \n",
    "    lm = model + f\"\"\"\\ \n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    ### Instruction:\n",
    "    {instruction_generate_burger}\n",
    "    ### Input:\n",
    "    {input_augmented}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    ans = lm + gen(\"ans\", max_tokens=150, stop='\\n')\n",
    "    \n",
    "    existing_data.append({\n",
    "        \"input\": initial_input,\n",
    "        \"input_augmented\": input_augmented,\n",
    "        \"output\": ans[\"ans\"],\n",
    "        \"expected\": expected,\n",
    "        \"output_NER\": used_items_str\n",
    "    })\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(existing_data, file, indent=4)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def canonicalize_order(order_str):\n",
    "    order_str = re.sub(r\",\\s*(size|container_type)='None'\", \"\", order_str)\n",
    "\n",
    "    def clean_topping(match):\n",
    "        inner = match.group(1)\n",
    "        inner = re.sub(r\",\\s*qualifier='None'\", \"\", inner)\n",
    "        inner = re.sub(r\",\\s*negation=False\", \"\", inner)\n",
    "        return f\"Topping({inner.strip()})\"\n",
    "    order_str = re.sub(r\"Topping\\((.*?)\\)\", clean_topping, order_str)\n",
    "\n",
    "    m = re.match(r\"(\\w+Order)\\((.*)\\)\", order_str)\n",
    "    if m:\n",
    "        order_type, params_str = m.groups()\n",
    "        params = []\n",
    "        current = \"\"\n",
    "        paren_count = 0\n",
    "        for ch in params_str:\n",
    "            if ch == \"(\":\n",
    "                paren_count += 1\n",
    "            elif ch == \")\":\n",
    "                paren_count -= 1\n",
    "            if ch == \",\" and paren_count == 0:\n",
    "                params.append(current.strip())\n",
    "                current = \"\"\n",
    "            else:\n",
    "                current += ch\n",
    "        if current.strip():\n",
    "            params.append(current.strip())\n",
    "        param_dict = {}\n",
    "        for param in params:\n",
    "            if '=' in param:\n",
    "                key, value = param.split('=', 1)\n",
    "                param_dict[key.strip()] = value.strip()\n",
    "        for key in list(param_dict.keys()):\n",
    "            if param_dict[key] == \"'None'\":\n",
    "                del param_dict[key]\n",
    "        if order_type == \"BurgerOrder\":\n",
    "            canon_order = [\"number\", \"main_dish_type\", \"toppings\"]\n",
    "        elif order_type == \"SideOrder\":\n",
    "            canon_order = [\"number\", \"side_type\", \"size\"]\n",
    "        elif order_type == \"DrinkOrder\":\n",
    "            canon_order = [\"number\", \"drink_type\", \"size\"]\n",
    "        else:\n",
    "            canon_order = list(param_dict.keys())\n",
    "        extra = sorted(set(param_dict.keys()) - set(canon_order))\n",
    "        final_order = canon_order + extra\n",
    "        new_params = []\n",
    "        for key in final_order:\n",
    "            if key in param_dict:\n",
    "                new_params.append(f\"{key}={param_dict[key]}\")\n",
    "        return f\"{order_type}(\" + \", \".join(new_params) + \")\"\n",
    "    else:\n",
    "        return order_str\n",
    "\n",
    "def split_orders(s):\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        s = s[1:-1]\n",
    "    orders = []\n",
    "    current = \"\"\n",
    "    paren_count = 0\n",
    "    for char in s:\n",
    "        if char == \"(\":\n",
    "            paren_count += 1\n",
    "        elif char == \")\":\n",
    "            paren_count -= 1\n",
    "        if char == \",\" and paren_count == 0:\n",
    "            orders.append(current.strip())\n",
    "            current = \"\"\n",
    "        else:\n",
    "            current += char\n",
    "    if current.strip():\n",
    "        orders.append(current.strip())\n",
    "    orders = [canonicalize_order(o) for o in orders]\n",
    "    return \"[\" + \", \".join(orders) + \"]\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSWFR2mo__ut",
    "outputId": "c0f086f5-98d7-4e8c-b703-0a90952b4ee3"
   },
   "source": [
    "import json\n",
    "\n",
    "def calculate_accuracy_and_save_mismatches(json_file, output_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    total = len(data)\n",
    "    correct = 0\n",
    "    mismatches = []\n",
    "\n",
    "    for item in data:\n",
    "        print(split_orders(item['output']))\n",
    "        output = split_orders(item['output'])\n",
    "        expected = split_orders(item['expected'])\n",
    "\n",
    "        if output == expected:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mismatches.append(item)\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(mismatches, outfile, indent=4)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "json_file = f'FoodOrderingDataset/output/Ablation_NER_Burger_Results_{model_name}.json'\n",
    "mismatch_file = f'FoodOrderingDataset/output/Ablation_NER_Burger_Mismatches_{model_name}.json'\n",
    "\n",
    "accuracy = calculate_accuracy_and_save_mismatches(json_file, mismatch_file)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Mismatches have been saved to: {mismatch_file}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "new_acl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
