{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "vM9f2Krba6_5",
    "outputId": "2385b8a3-c1a1-464c-dee3-6a23bd059fdc"
   },
   "source": [
    "import guidance\n",
    "from guidance import models, gen, one_or_more, select, zero_or_more, regex, optional, capture\n",
    "\n",
    "model_name = \"your_model_name_here\"\n",
    "\n",
    "model = models.LlamaCpp(f\"{model_name}.gguf\", n_gpu_layers=-1, n_ctx=1024)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@guidance(stateless=False)\n",
    "def burgerOrder(lm):\n",
    "    lm += 'number=' + select(number_values + ['1'], name='numberNames')\n",
    "    if lm['numberNames'] != '1':\n",
    "        number_values.remove(lm['numberNames'])\n",
    "    elif '1' in number_values:\n",
    "        number_values.remove(lm['numberNames'])\n",
    "    \n",
    "    if main_dish_type_values:\n",
    "        lm += select(\n",
    "            [\", main_dish_type='\" + select(main_dish_type_values, name=\"burgerTypeName\") + \"'\", \"\"],\n",
    "            name='burgerTypeFlag'\n",
    "        )\n",
    "        if lm['burgerTypeFlag'] != \"\":\n",
    "            main_dish_type_values.remove(lm['burgerTypeName'])\n",
    "    \n",
    "    if topping_values:\n",
    "        lm += select([\", toppings=[\", \"\"], name='toppingsFlag')\n",
    "    \n",
    "        if lm['toppingsFlag'] != \"\":\n",
    "            for i in topping_values[:]:\n",
    "                lm += topping()\n",
    "                if not topping_values:\n",
    "                    lm += ']'\n",
    "                    break\n",
    "                lm += select([\", \", \"]\"], name=\"finishedListToppings\")\n",
    "                if lm['finishedListToppings'] == \"]\":\n",
    "                    break\n",
    "    \n",
    "    return lm + \")\"\n",
    "\n",
    "\n",
    "@guidance(stateless=False)\n",
    "def topping(lm):\n",
    "    lm += \"Topping(name=\"\n",
    "    if topping_values:\n",
    "        lm += \"'\" + select(topping_values, name='toppingName') + \"'\"\n",
    "        topping_values.remove(lm['toppingName'])\n",
    "    \n",
    "    if quantity_values:\n",
    "        lm += select(\n",
    "            [\", qualifier='\" + select(quantity_values, name=\"qualifierName\") + \"'\", \"\"],\n",
    "            name='qualifierFlag'\n",
    "        )\n",
    "        if lm[\"qualifierFlag\"] != \"\":\n",
    "            quantity_values.remove(lm['qualifierName'])\n",
    "    \n",
    "    if not_values:\n",
    "        lm += select([\", negation=True\", \"\"], name='negationFlag')\n",
    "        if lm['negationFlag']:\n",
    "            print(not_values)\n",
    "            not_values.remove('not')\n",
    "    \n",
    "    lm += \")\"\n",
    "    return lm\n",
    "\n",
    "\n",
    "@guidance(stateless=False)\n",
    "def drinkOrder(lm):\n",
    "    lm += 'number=' + select(number_values + ['1'], name='numberNames')\n",
    "    if lm['numberNames'] != '1':\n",
    "        number_values.remove(lm['numberNames'])\n",
    "    elif '1' in number_values:\n",
    "        number_values.remove(lm['numberNames'])\n",
    "    \n",
    "    if drink_type_values:\n",
    "        lm += select(\n",
    "            [\", drink_type='\" + select(drink_type_values, name=\"drinkTypeName\") + \"'\", \"\"],\n",
    "            name='drinkTypeFlag'\n",
    "        )\n",
    "        if lm[\"drinkTypeFlag\"] != \"\":\n",
    "            drink_type_values.remove(lm['drinkTypeName'])\n",
    "    \n",
    "    if drink_size_values:\n",
    "        lm += select(\n",
    "            [\", size='\" + select(drink_size_values, name='drinkSizeName') + \"'\", \"\"],\n",
    "            name='drinkSizeFlag'\n",
    "        )\n",
    "        if lm[\"drinkSizeFlag\"] != \"\":\n",
    "            drink_size_values.remove(lm['drinkSizeName'])\n",
    "    \n",
    "    return lm + \")\"\n",
    "\n",
    "\n",
    "@guidance(stateless=False)\n",
    "def sideOrder(lm):\n",
    "    lm += 'number=' + select(number_values + ['1'], name='numberNames')\n",
    "    if lm['numberNames'] != '1':\n",
    "        number_values.remove(lm['numberNames'])\n",
    "    elif '1' in number_values:\n",
    "        number_values.remove(lm['numberNames'])\n",
    "    \n",
    "    if side_type_values:\n",
    "        lm += select(\n",
    "            [\", side_type='\" + select(side_type_values, name='sideTypeName') + \"'\", \"\"],\n",
    "            name='sideTypeFlag'\n",
    "        )\n",
    "        if lm['sideTypeFlag'] != '':\n",
    "            side_type_values.remove(lm['sideTypeName'])\n",
    "    \n",
    "    if side_size_values:\n",
    "        lm += select(\n",
    "            [\", size='\" + select(side_size_values, name='sideSizeName') + \"'\", \"\"],\n",
    "            name='sideSizeFlag'\n",
    "        )\n",
    "        if lm['sideSizeFlag'] != '':\n",
    "            side_size_values.remove(lm['sideSizeName'])\n",
    "    return lm + \")\"\n",
    "\n",
    "\n",
    "@guidance(stateless=False)\n",
    "def validOrderBurger(lm):\n",
    "    lm += \"[\"\n",
    "    first = True\n",
    "    for i in range(7):\n",
    "        choices = []\n",
    "        if main_dish_type_values:\n",
    "            choices.append(\"BurgerOrder(\")\n",
    "        if drink_type_values:\n",
    "            choices.append(\"DrinkOrder(\")\n",
    "        if side_type_values:\n",
    "            choices.append(\"SideOrder(\")\n",
    "    \n",
    "        if choices:\n",
    "            if not first:\n",
    "                lm += \", \"\n",
    "            else:\n",
    "                first = False\n",
    "    \n",
    "            lm += select(choices, name='choice')\n",
    "            if lm['choice'] == \"BurgerOrder(\":\n",
    "                lm += burgerOrder()\n",
    "            elif lm['choice'] == \"SideOrder(\":\n",
    "                lm += sideOrder()\n",
    "            else:\n",
    "                lm += drinkOrder()\n",
    "        else:\n",
    "            break\n",
    "    lm += \"]\"\n",
    "    return lm\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SZwQ3fBfbHnF"
   },
   "source": [
    "instruction_generate_burger = \"\"\"You are a helpful assistant. You have to take as input a customer order and output a list of the corresponding objects. You should use only the following classes in Python:\n",
    "      class Topping:\n",
    "            def __init__(self, name: str, qualifier: Optional[str] = None, negation: Optional[bool] = False) -> None:\n",
    "            \n",
    "      class BurgerOrder:\n",
    "            def __init__(self, number: int = 1, size: Optional[str] = None, main_dish_type: Optional[str] = None, toppings: Optional[List[Topping]] = None) -> None\n",
    "       \n",
    "      class DrinkOrder:\n",
    "            def __init__(self, number: int = 1, drink_type: Optional[str] = None, size: Optional[str] = None) -> None :\n",
    "      \n",
    "      class SideOrder:\n",
    "            def __init__(self, number: int = 1, side_type: Optional[str] = None, size: Optional[str] = None) -> None :\n",
    "      \n",
    "      The output should be a list of those objects.\\n\"\n",
    "      Here's an example:\n",
    "      'input': 'i would like a vegan burger with lettuce tomatoes and onions and a large order of sweet potato fries',\n",
    "      'output': '[BurgerOrder(number=1, main_dish_type='vegan_burger', toppings=[Topping(name='lettuce'), Topping(name='tomato'), Topping(name='onion')]), SideOrder(number=1, side_type='french_fries', size='large')]',\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.language import Language\n",
    "import os\n",
    "import re\n",
    "\n",
    "def parse_line(line):\n",
    "    parts = line.strip().split('\\t')\n",
    "    if len(parts) != 2:\n",
    "        return None, None\n",
    "    phrase, category = parts\n",
    "    return phrase, category.strip()\n",
    "\n",
    "def get_all_ngrams(tokens, max_n):\n",
    "    ngrams = set()\n",
    "    for n in range(1, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = \" \".join([token.text for token in tokens[i:i + n]])\n",
    "            ngrams.add(ngram)\n",
    "    return ngrams\n",
    "\n",
    "def init_pipeline(dataset=\"burger\"):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    ner_ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"phrase_matcher_attr\": \"LOWER\"})\n",
    "    \n",
    "    def read_file_categories(food_type):\n",
    "        file_path = f\"FoodOrderingDataset/data/{food_type}/alias\"\n",
    "        text_files = [f for f in os.listdir(file_path) if f.endswith('.txt')]\n",
    "        patterns = []\n",
    "        for file in text_files:\n",
    "            path_to_file = os.path.join(file_path, file)\n",
    "            with open(path_to_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        phrase, category_info = parse_line(line)\n",
    "                        if phrase and category_info:\n",
    "                            schema = {\"pattern\": phrase, \"label\": category_info}\n",
    "                            patterns.append(schema)\n",
    "        return patterns\n",
    "\n",
    "    category_patterns = read_file_categories(dataset)\n",
    "    ner_ruler.add_patterns(category_patterns)\n",
    "\n",
    "    global DRINK_KEYWORDS, SIDE_KEYWORDS\n",
    "    DRINK_KEYWORDS = {pat[\"pattern\"].lower() for pat in category_patterns if pat[\"label\"].startswith(\"DRINK_TYPE\")}\n",
    "    SIDE_KEYWORDS = {pat[\"pattern\"].lower() for pat in category_patterns if pat[\"label\"].startswith(\"SIDE_TYPE\")}\n",
    "    \n",
    "    nlp.add_pipe(\"disambiguate_size\", after=\"ner\")\n",
    "    \n",
    "    return nlp\n",
    "\n",
    "@Language.component(\"disambiguate_size\")\n",
    "def disambiguate_size(doc):\n",
    "    new_ents = []\n",
    "    \n",
    "    max_drink = max((len(phrase.split()) for phrase in DRINK_KEYWORDS), default=1)\n",
    "    max_side = max((len(phrase.split()) for phrase in SIDE_KEYWORDS), default=1)\n",
    "    max_n = max(max_drink, max_side)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if \"SIZE\" in ent.label_:\n",
    "            start = max(0, ent.start)\n",
    "            end = min(len(doc), ent.end + 0)\n",
    "            context_tokens = []\n",
    "            while end <= len(doc):\n",
    "                context_tokens = [token for token in doc[start:end]]\n",
    "                context_ngrams = get_all_ngrams(context_tokens, max_n)\n",
    "                if context_ngrams & DRINK_KEYWORDS:\n",
    "                    new_ent = spacy.tokens.Span(doc, ent.start, ent.end, label=\"DRINK_\" + ent.label_)\n",
    "                    new_ents.append(new_ent)\n",
    "                    break\n",
    "                elif context_ngrams & SIDE_KEYWORDS:\n",
    "                    new_ent = spacy.tokens.Span(doc, ent.start, ent.end, label=\"SIDE_\" + ent.label_)\n",
    "                    new_ents.append(new_ent)\n",
    "                    break\n",
    "                else:\n",
    "                    if end == len(doc) and start == ent.start:\n",
    "                        start -= 1\n",
    "                    else:\n",
    "                        end += 1\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = tuple(new_ents)\n",
    "    return doc\n",
    "\n",
    "def process_NER(input_order):\n",
    "    found_categories = []\n",
    "    doc = nlp(input_order)\n",
    "    for ent in doc.ents:\n",
    "        found_categories.append((ent.text, ent.label_))\n",
    "    return found_categories\n",
    "\n",
    "nlp = init_pipeline()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "p87BpXABbPpa",
    "outputId": "ad855072-afae-4f76-a5ba-c3860dd3bcc5"
   },
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "file_path = f'FoodOrderingDataset/output/{model_name}-burger-NER-Disambiguation.json'\n",
    "\n",
    "existing_data = []\n",
    "\n",
    "with open('FoodOrderingDataset/processed_data/burger_dataset_disambiguation.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "input_list = []\n",
    "for obj in data:\n",
    "    input_value = obj.get(\"input\", \"No input key found\")\n",
    "    output_value = obj.get(\"output_extract\", \"No output key found\")\n",
    "    output_generate = obj.get(\"output_generate\", \"No output key found\")\n",
    "    \n",
    "    used_items_value = process_NER(input_value)\n",
    "    used_items_value_decoupled = [f\"{ent_text} - {ent_label}\" for ent_text, ent_label in used_items_value]\n",
    "    used_items_str = ', '.join(used_items_value_decoupled).lower()\n",
    "\n",
    "    input_augmented_file = input_value + \"\\nItems Found: \" + used_items_str\n",
    "    input_list.append((input_value, input_augmented_file, output_generate, used_items_value, used_items_str))\n",
    "\n",
    "for i in range(len(input_list)):\n",
    "    if i > 130:\n",
    "        break\n",
    "    (initial_input, input_augmented, expected, used_items_value, used_items_str) = input_list[i]\n",
    "    \n",
    "    lm = model + f\"\"\"\\ \n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    ### Instruction:\n",
    "    {instruction_generate_burger}\n",
    "    ### Input:\n",
    "    {input_augmented}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    items = []\n",
    "    for ent_text, ent_label in used_items_value:\n",
    "        if \"(\" in ent_label:\n",
    "            base_label = ent_label.split(\"(\")[0].lower()\n",
    "            canonical_text = ent_label.split(\"(\")[1].lower()[:-1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        items.append((base_label, canonical_text))\n",
    "    \n",
    "    items_dict = defaultdict(list)\n",
    "    for item_type, item_value in items:\n",
    "        items_dict[item_type].append(item_value)\n",
    "    \n",
    "    print(items_dict)\n",
    "    \n",
    "    keys_to_extract = ['drink_size', 'side_size', 'main_dish_type', 'topping', 'quantity', 'not', 'drink_type', 'side_type', 'number']\n",
    "    \n",
    "    for key in keys_to_extract:\n",
    "        globals()[f\"{key}_values\"] = items_dict.get(key, [])\n",
    "        print(key, globals()[f\"{key}_values\"])\n",
    "    \n",
    "    ans = lm + capture(validOrderBurger(), \"answer\")\n",
    "    \n",
    "    existing_data.append({\n",
    "        \"input\": initial_input,\n",
    "        \"input_augmented\": input_augmented,\n",
    "        \"output\": ans[\"answer\"],\n",
    "        \"expected\": expected,\n",
    "        \"output_NER\": used_items_str\n",
    "    })\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(existing_data, file, indent=4)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def split_orders(s):\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        s = s[1:-1]\n",
    "    orders = []\n",
    "    current = \"\"\n",
    "    paren_count = 0\n",
    "    for char in s:\n",
    "        if char == \"(\":\n",
    "            paren_count += 1\n",
    "        elif char == \")\":\n",
    "            paren_count -= 1\n",
    "        if char == \",\" and paren_count == 0:\n",
    "            orders.append(current.strip())\n",
    "            current = \"\"\n",
    "        else:\n",
    "            current += char\n",
    "    if current.strip():\n",
    "        orders.append(current.strip())\n",
    "    return sorted(orders)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSWFR2mo__ut",
    "outputId": "c0f086f5-98d7-4e8c-b703-0a90952b4ee3"
   },
   "source": [
    "import json\n",
    "\n",
    "def calculate_accuracy_and_save_mismatches(json_file, output_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    total = len(data)\n",
    "    correct = 0\n",
    "    mismatches = []\n",
    "\n",
    "    for item in data:\n",
    "        print(split_orders(item['output']))\n",
    "        output = split_orders(item['output'])\n",
    "        expected = split_orders(item['expected'])\n",
    "\n",
    "        if output == expected:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mismatches.append(item)\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(mismatches, outfile, indent=4)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "json_file = f'FoodOrderingDataset/output/{model_name}-burger-NER-Disambiguation.json'\n",
    "mismatch_file = f'FoodOrderingDataset/output/{model_name}-burger-NER-Disambiguation_processed_mismatches.json'\n",
    "\n",
    "accuracy = calculate_accuracy_and_save_mismatches(json_file, mismatch_file)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Mismatches have been saved to: {mismatch_file}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "new_acl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
