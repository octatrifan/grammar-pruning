{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "vM9f2Krba6_5",
        "outputId": "2385b8a3-c1a1-464c-dee3-6a23bd059fdc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25635a3a82294ffab3229c026ab3654a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import guidance\n",
        "from guidance import models, gen, one_or_more, select, zero_or_more, regex, optional, capture\n",
        "\n",
        "model_name = \"your_model_name_here\"\n",
        "\n",
        "model = models.LlamaCpp(f\"{model_name}.gguf\", n_gpu_layers=-1, n_ctx=2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpyQ_wIKbEoj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\d'\n",
            "/tmp/ipykernel_134591/2586245235.py:4: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lm += select([\"number=\"+regex(\"\\d+\"), \"\"], name='numberFlag')\n",
            "<unknown>:4: SyntaxWarning: invalid escape sequence '\\d'\n"
          ]
        }
      ],
      "source": [
        "@guidance(stateless=False)\n",
        "def drinkOrderCoffee(lm):\n",
        "    lm += \"DrinkOrder(\"\n",
        "    lm += select([\"number=\"+regex(\"\\d+\"), \"\"], name='numberFlag')\n",
        "    if drink_type_values:\n",
        "      lm += select([\", drink_type='\" + select(drink_type_values, name='drinkTypeName')+\"'\", \"\"], name='drinkTypeFlag')\n",
        "      if lm['drinkTypeFlag'] != \"\":\n",
        "        drink_type_values.remove(lm['drinkTypeName'])\n",
        "\n",
        "    if roast_type_values:\n",
        "      lm += select([\", roast_type='\"+select(roast_type_values, name='roastTypeName')+\"'\", \"\"], name='drinkTypeFlag')\n",
        "      if lm['drinkTypeFlag'] != \"\":\n",
        "        roast_type_values.remove(lm['roastTypeName'])\n",
        "\n",
        "    if size_values:\n",
        "      lm += select([\", size='\"+select(size_values, name='sizeName')+\"'\", \"\"], name='sizeFlag')\n",
        "      if lm['sizeFlag'] != \"\":\n",
        "        size_values.remove(lm['sizeName'])\n",
        "\n",
        "    if style_values:\n",
        "      lm += select([\", style='\"+select(style_values, name='styleName')+\"'\", \"\"], name='styleFlag')\n",
        "      if lm['styleFlag'] != \"\":\n",
        "        style_values.remove(lm['styleName'])\n",
        "\n",
        "    if topping_values:\n",
        "      lm += select([\", toppings=[\", \"\"], name='toppingsFlag')\n",
        "      if lm['toppingsFlag']:\n",
        "        for i in topping_values[:]:\n",
        "          lm += toppingCoffee()\n",
        "          if not topping_values:\n",
        "            lm += ']'\n",
        "            break\n",
        "          lm += select([\", \", \"]\"], name=\"finishedListToppings\")\n",
        "          if lm['finishedListToppings'] == \"]\":\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    return lm + \")\"\n",
        "\n",
        "@guidance(stateless=False)\n",
        "def toppingCoffee(lm):\n",
        "  lm += \"Topping(name=\"\n",
        "  if topping_values:\n",
        "    lm += \"'\" + select(topping_values, name='toppingName') + \"'\"\n",
        "    topping_values.remove(lm['toppingName'])\n",
        "\n",
        "  if quantity_values:\n",
        "    lm += select([\", qualifier='\" + select(quantity_values, name='qualifierName') + \"'\", \"\"], name='qualifierFlag')\n",
        "    if lm[\"qualifierFlag\"] != \"\":\n",
        "      quantity_values.remove(lm['qualifierName'])\n",
        "\n",
        "  if not_values:\n",
        "    lm += select([\", negation=True\", \"\"], name='negationFlag')\n",
        "    if lm['negationFlag'] != \"\":\n",
        "      not_values.remove('not')\n",
        "\n",
        "  lm += \")\"\n",
        "  return lm\n",
        "\n",
        "@guidance(stateless=False)\n",
        "def validOrderCoffee(lm):\n",
        "  lm += \"[\"\n",
        "  first = True\n",
        "  for i in range(7):\n",
        "    if drink_type_values:\n",
        "      if not first:\n",
        "        lm += select(\", \", \"\")\n",
        "      else:\n",
        "        first = False\n",
        "\n",
        "      lm += drinkOrderCoffee()\n",
        "    else:\n",
        "      break\n",
        "  return lm +']'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SZwQ3fBfbHnF"
      },
      "outputs": [],
      "source": [
        "instruction_generate_coffee = \"\"\"You are a helpful assistant. You have to take as input a customer order and output a list of the corresponding objects. You should use only the following classes in Python:\n",
        "class Topping:\n",
        "      def __init__(self, name: str, qualifier: Optional[str] = None, negation: Optional[bool] = False) -> None:\n",
        "\n",
        "class DrinkOrder:\n",
        "      def __init__(self, number: int = 1, drink_type: Optional[str] = None, size: Optional[str] = None, style: Optional[str] = None, roast_type: Optional[str] = None, toppings: Optional[List[Topping]] = None) -> None:\n",
        "\n",
        "The output should be a list of those objects.\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dWZpodUbqbP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import spacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "from spacy import displacy\n",
        "import os\n",
        "import re\n",
        "\n",
        "def parse_line(line):\n",
        "    parts = line.strip().split('\\t')\n",
        "    if len(parts) != 2:\n",
        "        return None, None\n",
        "\n",
        "    phrase, category = parts\n",
        "\n",
        "    return phrase, category.strip()\n",
        "\n",
        "def init_pipeline(dataset = \"coffee\"):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  ner_ruler = nlp.add_pipe(\"entity_ruler\",\n",
        "                      before=\"ner\",\n",
        "                      config={\"phrase_matcher_attr\": \"LOWER\"})\n",
        "\n",
        "  def read_file_categories(food_type):\n",
        "      file_path = f\"FoodOrderingDataset/data/{food_type}/alias\"\n",
        "\n",
        "      text_files = [f for f in os.listdir(file_path) if f.endswith('.txt')]\n",
        "\n",
        "      patterns = []\n",
        "\n",
        "      for file in text_files:\n",
        "          path_to_file = f\"{file_path}/{file}\"\n",
        "          with open(path_to_file, 'r') as file:\n",
        "              for line in file:\n",
        "                  if line.strip():\n",
        "                      phrase, category_info = parse_line(line)\n",
        "                      if phrase and category_info:\n",
        "                          schema = {}\n",
        "                          schema[\"pattern\"] = phrase\n",
        "                          schema[\"label\"] = category_info\n",
        "                          patterns.append(schema)\n",
        "\n",
        "      return patterns\n",
        "\n",
        "  category_patterns = read_file_categories(dataset)\n",
        "\n",
        "  ner_ruler.add_patterns(category_patterns)\n",
        "  return nlp\n",
        "\n",
        "\n",
        "nlp = init_pipeline()\n",
        "\n",
        "def process_NER(input_order):\n",
        "    found_categories = []\n",
        "\n",
        "    doc = nlp(input_order)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        found_categories.append((ent.text, ent.label_))\n",
        "\n",
        "    return found_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "p87BpXABbPpa",
        "outputId": "ad855072-afae-4f76-a5ba-c3860dd3bcc5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c790f4e48a274ed59292c9f5dfe6a2c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('a', 'number(1)'), ('regular', 'SIZE(regular)'), ('latte', 'DRINK_TYPE(latte)'), ('cinnamon', 'TOPPING(cinnamon)'), ('iced', 'STYLE(iced)'), ('one extra espresso shot', 'TOPPING(ESPRESSO_SHOT_1)')]\n",
            "defaultdict(<class 'list'>, {'number': ['1'], 'size': ['regular'], 'drink_type': ['latte'], 'topping': ['cinnamon', 'espresso_shot_1'], 'style': ['iced']})\n",
            "topping ['cinnamon', 'espresso_shot_1']\n",
            "size ['regular']\n",
            "number ['1']\n",
            "drink_type ['latte']\n",
            "roast_type []\n",
            "not []\n",
            "style ['iced']\n",
            "quantity []\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_134591/2586245235.py:4: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lm += select([\"number=\"+regex(\"\\d+\"), \"\"], name='numberFlag')\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m items_dict\u001b[38;5;241m.\u001b[39mget(key, [])\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key, \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_values\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 72\u001b[0m ans \u001b[38;5;241m=\u001b[39m lm \u001b[38;5;241m+\u001b[39m capture(validOrderCoffee(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# print(ans[\"answer\"])\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Append the new data to the existing data list\u001b[39;00m\n\u001b[1;32m     76\u001b[0m existing_data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m:initial_input, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_augmented\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: ans[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected\u001b[39m\u001b[38;5;124m\"\u001b[39m:expected, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_NER\u001b[39m\u001b[38;5;124m\"\u001b[39m: used_items_str})\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/models/_model.py:1212\u001b[0m, in \u001b[0;36mModel.__add__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m# run stateful functions\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     lm\u001b[38;5;241m.\u001b[39m_update_trace_node(lm\u001b[38;5;241m.\u001b[39m_id, lm\u001b[38;5;241m.\u001b[39m_parent_id, StatefulGuidanceInput(value\u001b[38;5;241m=\u001b[39mvalue))\n\u001b[0;32m-> 1212\u001b[0m     out \u001b[38;5;241m=\u001b[39m value(lm)\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m   1215\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA guidance function returned `None`, not a model object! Did you forget to return the new lm at the end of your function?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1216\u001b[0m         )\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/_grammar.py:60\u001b[0m, in \u001b[0;36mRawFunction.__call__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/library/_capture.py:11\u001b[0m, in \u001b[0;36mcapture\u001b[0;34m(lm, value, name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     start_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lm)\n\u001b[0;32m---> 11\u001b[0m     lm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lm\u001b[38;5;241m.\u001b[39mset(name, \u001b[38;5;28mstr\u001b[39m(lm)[start_len:])\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/models/_model.py:1212\u001b[0m, in \u001b[0;36mModel.__add__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m# run stateful functions\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     lm\u001b[38;5;241m.\u001b[39m_update_trace_node(lm\u001b[38;5;241m.\u001b[39m_id, lm\u001b[38;5;241m.\u001b[39m_parent_id, StatefulGuidanceInput(value\u001b[38;5;241m=\u001b[39mvalue))\n\u001b[0;32m-> 1212\u001b[0m     out \u001b[38;5;241m=\u001b[39m value(lm)\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m   1215\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA guidance function returned `None`, not a model object! Did you forget to return the new lm at the end of your function?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1216\u001b[0m         )\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/_grammar.py:60\u001b[0m, in \u001b[0;36mRawFunction.__call__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
            "Cell \u001b[0;32mIn[2], line 73\u001b[0m, in \u001b[0;36mvalidOrderCoffee\u001b[0;34m(lm)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m   lm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m drinkOrderCoffee()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/models/_model.py:1212\u001b[0m, in \u001b[0;36mModel.__add__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m# run stateful functions\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     lm\u001b[38;5;241m.\u001b[39m_update_trace_node(lm\u001b[38;5;241m.\u001b[39m_id, lm\u001b[38;5;241m.\u001b[39m_parent_id, StatefulGuidanceInput(value\u001b[38;5;241m=\u001b[39mvalue))\n\u001b[0;32m-> 1212\u001b[0m     out \u001b[38;5;241m=\u001b[39m value(lm)\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m   1215\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA guidance function returned `None`, not a model object! Did you forget to return the new lm at the end of your function?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1216\u001b[0m         )\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/_grammar.py:60\u001b[0m, in \u001b[0;36mRawFunction.__call__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
            "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mdrinkOrderCoffee\u001b[0;34m(lm)\u001b[0m\n\u001b[1;32m     24\u001b[0m     style_values\u001b[38;5;241m.\u001b[39mremove(lm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyleName\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m topping_values:\n\u001b[0;32m---> 27\u001b[0m   lm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m select([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, toppings=[\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoppingsFlag\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m lm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoppingsFlag\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m topping_values[:]:\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/models/_model.py:1207\u001b[0m, in \u001b[0;36mModel.__add__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, GrammarFunction):\n\u001b[1;32m   1206\u001b[0m     lm\u001b[38;5;241m.\u001b[39m_update_trace_node(lm\u001b[38;5;241m.\u001b[39m_id, lm\u001b[38;5;241m.\u001b[39m_parent_id, StatelessGuidanceInput(value\u001b[38;5;241m=\u001b[39mvalue))\n\u001b[0;32m-> 1207\u001b[0m     out \u001b[38;5;241m=\u001b[39m lm\u001b[38;5;241m.\u001b[39m_run_stateless(value)\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m# run stateful functions\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     lm\u001b[38;5;241m.\u001b[39m_update_trace_node(lm\u001b[38;5;241m.\u001b[39m_id, lm\u001b[38;5;241m.\u001b[39m_parent_id, StatefulGuidanceInput(value\u001b[38;5;241m=\u001b[39mvalue))\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/models/_model.py:1413\u001b[0m, in \u001b[0;36mModel._run_stateless\u001b[0;34m(self, stateless_function, temperature, top_p, n)\u001b[0m\n\u001b[1;32m   1410\u001b[0m delayed_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;66;03m# last_is_generated = False\u001b[39;00m\n\u001b[0;32m-> 1413\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m gen_obj:\n\u001b[1;32m   1414\u001b[0m \n\u001b[1;32m   1415\u001b[0m     \u001b[38;5;66;03m# we make everything full probability if we are not computing uncertainty\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;66;03m# if not self.engine.compute_log_probs:\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;66;03m#     chunk.new_bytes_prob = 1.0\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;66;03m# convert the bytes to a string (delaying if we don't yet have a valid unicode string)\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m     lm\u001b[38;5;241m.\u001b[39mtoken_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mnew_token_count\n\u001b[1;32m   1421\u001b[0m     chunk\u001b[38;5;241m.\u001b[39mnew_bytes \u001b[38;5;241m=\u001b[39m delayed_bytes \u001b[38;5;241m+\u001b[39m chunk\u001b[38;5;241m.\u001b[39mnew_bytes\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/models/_model.py:441\u001b[0m, in \u001b[0;36mEngine.__call__\u001b[0;34m(self, prompt, grammar, ensure_bos_token, echo)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 441\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_logits(token_ids\u001b[38;5;241m=\u001b[39mtokens)\n\u001b[1;32m    442\u001b[0m     logits_lat_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;66;03m# Permanently fall-back to get_next_token if get_logits is not implemented\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/new_acl/lib/python3.12/site-packages/guidance/models/llama_cpp/_llama_cpp.py:216\u001b[0m, in \u001b[0;36mLlamaCppEngine.get_logits\u001b[0;34m(self, token_ids)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m+\u001b[39m n_tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(token_ids):\n\u001b[1;32m    214\u001b[0m     batch\u001b[38;5;241m.\u001b[39mlogits[n_tokens \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m ret \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_obj\u001b[38;5;241m.\u001b[39mctx, batch)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mengine_input_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_tokens\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "file_path = f'FoodOrderingDataset/output/{model_name}-coffee-NER.json'\n",
        "\n",
        "existing_data=[]\n",
        "\n",
        "with open('FoodOrderingDataset/processed_data/coffee_dataset.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "input_list = []\n",
        "for obj in data:\n",
        "    input_value = obj.get(\"input\", \"No input key found\")\n",
        "    output_value = obj.get(\"output_extract\", \"No output key found\")\n",
        "    output_generate = obj.get(\"output_generate\", \"No output key found\")\n",
        "    used_items_value = process_NER(input_value)\n",
        "    used_items_value_decoupled = [x[0] + ' - ' + x[1] for x in used_items_value]\n",
        "    used_items_str = ', '.join(used_items_value_decoupled).lower()\n",
        "\n",
        "    input_augmented_file = input_value + \"\\nItems Found: \" + used_items_str\n",
        "    input_list.append((input_value, input_augmented_file, output_generate, used_items_value, used_items_str))\n",
        "\n",
        "for i in range(len(input_list)):\n",
        "    if i > 130:\n",
        "        break\n",
        "    (initial_input, input, expected, used_items_value, used_items_str) = input_list[i]\n",
        "    lm = model + f'''\\\n",
        "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "    ### Instruction:\n",
        "    {instruction_generate_coffee}\n",
        "    ### Input:\n",
        "    {input}\n",
        "\n",
        "    ### Response:\n",
        "    '''\n",
        "\n",
        "    items = []\n",
        "    print(used_items_value)\n",
        "    for item in used_items_value:\n",
        "      match = re.search(r'(\\w+)\\(([^)]+)\\)', item[1])\n",
        "      if match:\n",
        "        items.append((match.group(1), match.group(2)))\n",
        "    items_dict = defaultdict(list)\n",
        "    for item_type, item_value in items:\n",
        "        items_dict[item_type.lower()].append(item_value.lower())\n",
        "\n",
        "    print(items_dict)\n",
        "    items_dict = dict(items_dict)\n",
        "    keys_to_extract = ['topping', 'size', 'number', 'drink_type', 'roast_type', 'not', 'style', 'quantity']\n",
        "\n",
        "    for key in keys_to_extract:\n",
        "        globals()[f\"{key}_values\"] = items_dict.get(key, [])\n",
        "        print(key, globals()[f\"{key}_values\"])\n",
        "\n",
        "    ans = lm + capture(validOrderCoffee(), \"answer\")\n",
        "\n",
        "    existing_data.append({\"input\":initial_input, \"input_augmented\": input, \"output\": ans[\"answer\"], \"expected\":expected, \"output_NER\": used_items_str})\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    json.dump(existing_data, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def remove_duplicate_toppings(order_str: str) -> str:\n",
        "    pattern = r\"(toppings=\\[)(.*?)(\\])\"\n",
        "    \n",
        "    def dedupe(match: re.Match) -> str:\n",
        "        prefix = match.group(1)\n",
        "        content = match.group(2)\n",
        "        suffix = match.group(3)\n",
        "        \n",
        "        topping_pattern = r\"Topping\\([^)]*\\)\"\n",
        "        topping_items = re.findall(topping_pattern, content)\n",
        "        \n",
        "        seen = set()\n",
        "        unique_toppings = []\n",
        "        for item in topping_items:\n",
        "            if item not in seen:\n",
        "                seen.add(item)\n",
        "                unique_toppings.append(item)\n",
        "        \n",
        "        new_content = \", \".join(unique_toppings)\n",
        "        return f\"{prefix}{new_content}{suffix}\"\n",
        "    \n",
        "    cleaned_str = re.sub(pattern, dedupe, order_str, flags=re.DOTALL)\n",
        "    if cleaned_str[-4:] != \")])]\" and \"Topping\" in cleaned_str: cleaned_str += \")]\"\n",
        "    return cleaned_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSWFR2mo__ut",
        "outputId": "c0f086f5-98d7-4e8c-b703-0a90952b4ee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "[drinkorder(number=1, drink_type='latte', roast_type='light_roast', size='large', toppings=[topping(name='honey'), topping(name='espresso_shot_1'), topping(name='caramel_syrup')]), drinkorder(number=1, drink_type='cappuccino', size='regular')])]\n",
            "[drinkorder(number=1, drink_type='latte', roast_type='light_roast', size='regular', toppings=[topping(name='espresso_shot_1'), topping(name='honey')]), drinkorder(number=1, drink_type='cappuccino', size='large', toppings=[topping(name='caramel_syrup')])]\n",
            "\n",
            "\n",
            "\n",
            "[drinkorder(number=1, drink_type='latte', roast_type='french', size='large', toppings=[topping(name='caramel_syrup')]), drinkorder(number=1, drink_type='americano', roast_type='dark_roast', size='regular')])]\n",
            "[drinkorder(number=1, drink_type='latte', roast_type='french', size='large'), drinkorder(number=1, drink_type='americano', roast_type='dark_roast', size='regular', toppings=[topping(name='caramel_syrup')])]\n",
            "\n",
            "\n",
            "\n",
            "[drinkorder(number=1, drink_type='latte', roast_type='dark_roast', size='large', style='skinny', toppings=[topping(name='raspberry_syrup'), topping(name='whipped_cream')]), drinkorder(number=1, drink_type='americano', roast_type='medium_roast', size='small')])]\n",
            "[drinkorder(number=1, drink_type='latte', roast_type='medium_roast', size='large', style='skinny', toppings=[topping(name='raspberry_syrup')]), drinkorder(number=1, drink_type='americano', roast_type='dark_roast', size='small', toppings=[topping(name='whipped_cream')])]\n",
            "\n",
            "\n",
            "\n",
            "[drinkorder(number=1, drink_type='cappuccino', roast_type='dark_roast', size='large', toppings=[topping(name='caramel_syrup'), topping(name='vanilla_syrup')])]\n",
            "[drinkorder(number=1, drink_type='cappuccino', roast_type='dark_roast', size='large', toppings=[topping(name='vanilla_syrup'), topping(name='caramel_syrup')])]\n",
            "\n",
            "\n",
            "\n",
            "[drinkorder(number=1, drink_type='espresso', size='large', toppings=[topping(name='cinnamon'), topping(name='drizzles')])]\n",
            "[drinkorder(number=2, drink_type='espresso', size='large', toppings=[topping(name='cinnamon'), topping(name='drizzles')])]\n",
            "\n",
            "\n",
            "\n",
            "[drinkorder(number=1, drink_type='drip_coffee', roast_type='french', size='large'), drinkorder(number=1, drink_type='americano')]\n",
            "[drinkorder(number=1, drink_type='americano', roast_type='french', size='large')]\n",
            "\n",
            "\n",
            "\n",
            "[drinkorder(number=1, drink_type='hot_chocolate', size='regular', toppings=[topping(name='vanilla_syrup'), topping(name='whipped_cream')]), drinkorder(number=1, drink_type='drip_coffee', size='small')])]\n",
            "[drinkorder(number=1, drink_type='hot_chocolate', size='small', toppings=[topping(name='whipped_cream')]), drinkorder(number=1, drink_type='drip_coffee', size='regular', toppings=[topping(name='vanilla_syrup')])]\n",
            "\n",
            "\n",
            "\n",
            "[drinkorder(number=1, drink_type='cappuccino', roast_type='italian', size='large', style='decaf', toppings=[topping(name='vanilla_syrup'), topping(name='crumbles')])]\n",
            "[drinkorder(number=1, roast_type='continental_roast', size='large', style='decaf', toppings=[topping(name='vanilla_syrup')]), drinkorder(number=1, drink_type='cappuccino', roast_type='italian', size='small', style='skinny', toppings=[topping(name='crumbles')])]\n",
            "\n",
            "\n",
            "\n",
            "[drinkorder(number=1, drink_type='drip_coffee', roast_type='french', size='small', style='decaf', toppings=[topping(name='hazelnut_syrup'), topping(name='drizzles')])]\n",
            "[drinkorder(number=1, drink_type='drip_coffee', roast_type='french', size='small', style='iced', toppings=[topping(name='hazelnut_syrup'), topping(name='drizzles')])]\n",
            "Accuracy: 91.09%\n",
            "Mismatches have been saved to: FoodOrderingDataset/output/tuned_Qwen2.5-Coder-0.5B-Instruct-bnb-4bit-coffee-NER_processed_mismatches.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def calculate_accuracy_and_save_mismatches(json_file, output_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    total = len(data)\n",
        "    correct = 0\n",
        "    mismatches = []\n",
        "\n",
        "    for item in data:\n",
        "        output = item['output']\n",
        "        expected = item['expected'].lower()\n",
        "\n",
        "        output = remove_duplicate_toppings(output).lower()\n",
        "\n",
        "        if output == expected:\n",
        "            correct += 1\n",
        "        else:\n",
        "            print('\\n\\n')\n",
        "            print(output)\n",
        "            print(expected)\n",
        "            mismatches.append(item)\n",
        "\n",
        "    accuracy = (correct / total) * 100\n",
        "\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        json.dump(mismatches, outfile, indent=4)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "json_file = f'FoodOrderingDataset/output/{model_name}-coffee-NER.json'\n",
        "mismatch_file = f'FoodOrderingDataset/output/{model_name}-coffee-NER_processed_mismatches.json'\n",
        "\n",
        "accuracy = calculate_accuracy_and_save_mismatches(json_file, mismatch_file)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Mismatches have been saved to: {mismatch_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ACL_Inference",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
