# ğŸš€ Grammar Pruning: Enabling Low-Latency Zero-Shot Task-Oriented Semantic Parsing for Edge AI

This repository hosts the official implementation and datasets for our paper, **"Grammar Pruning: Enabling Low-Latency Zero-Shot Task-Oriented Language Models for Edge AI"**. Our innovative approach enables precise, real-time semantic parsing directly on resource-constrained edge devices. ğŸ“±âš™ï¸

## Abstract
Edge deployment of task-oriented semantic parsers demands high accuracy under tight latency and memory budgets. We present Grammar Pruning, a lightweight zero-shot framework that begins with a user-defined schema of API calls and couples a rule-based entity extractor with an iterative grammar-constrained decoder: extracted items dynamically prune the context-free grammar, limiting generation to only those intents, slots, and values that remain plausible at each step. This aggressive search-space reduction both eliminates hallucinations and slashes decoding time. On the adapted FoodOrdering, APIMixSNIPS, and APIMixATIS benchmarks, Grammar Pruning with small language models achieves an average execution accuracy of over 90\%â€”rivaling State-of-the-Art, cloud-based solutionsâ€”while sustaining at least 2x lower end-to-end latency than existing methods. By requiring nothing beyond the domainâ€™s full API schema values yet delivering precise, real-time natural-language understanding, Grammar Pruning positions itself as a practical building block for future edge-AI applications that cannot rely on large models or cloud offloading.

![Grammar Pruning Illustration](https://github.com/user-attachments/assets/bbc8c86f-20c9-45d1-8bed-30a5cae6150e)


*Figure 1: Grammar Pruning dynamically prunes generation grammars, ensuring fast, accurate, and hallucination-free semantic parsing.*

## ğŸ“Œ Key Features

* âœ… **Zero-Shot Adaptability**: No fine-tuning needed when deploying to new domains.
* ğŸ§¹ **Dynamic Grammar Reduction**: Limits responses strictly to valid schema elements, eliminating inaccuracies.
* ğŸ› ï¸ **Edge-Optimized**: Designed for minimal computational resources, suitable for small language models.
* âš¡ **High Accuracy, Low Latency**: Consistently achieves over 90% accuracy, delivering results twice as fast as comparable methods.

## ğŸ“Š Experimental Highlights

| Dataset                  | Model Size | Accuracy (%) | Latency (s) |
| ------------------------ | ---------- | ------------ | ----------- |
| ğŸ” FoodOrdering (Burger) | 1.5B       | 96.2%        | <1s         |
| â˜• FoodOrdering (Coffee)  | 1.5B       | 91.1%        | <1s         |
| ğŸ¤ APIMixSNIPS           | 4B         | 96.1%        | <1s         |
| âœˆï¸ APIMixATIS            | 4B         | 92.2%        | <1s         |

## ğŸ“š Datasets

We provide meticulously adapted versions of three prominent semantic parsing benchmarks:

* ğŸ• [FoodOrderingDataset](https://github.com/amazon-science/food-ordering-semantic-parsing-dataset)
* ğŸ¤ **API**[MixSNIPS](https://github.com/VinAIResearch/MISCA/tree/main/data/mixsnips)
* âœˆï¸ **API**[MixATIS](https://github.com/VinAIResearch/MISCA/tree/main/data/mixsnips)

Datasets are formatted as Python API calls for seamless parsing and easy validation. Schemas for each datasets are included.

## ğŸ—‚ï¸ Repository Structure

```
â”œâ”€â”€ APIMixATIS
â”‚   â”œâ”€â”€ atis_data.json
â”‚   â”œâ”€â”€ atis_data_augmented.json
â”‚   â””â”€â”€ atis_data_schema.json
â”œâ”€â”€ APIMixSNIPS
â”‚   â”œâ”€â”€ snips_data.json
â”‚   â”œâ”€â”€ snips_data_augmented.json
â”‚   â””â”€â”€ snips_data_schema.json
â”œâ”€â”€ FoodOrderingDataset
â”‚   â”œâ”€â”€ data
â”‚   â””â”€â”€ scripts
â”œâ”€â”€ Implementation
â”‚   â”œâ”€â”€ APIMixATIS & APIMixSNIPS
â”‚   â”‚   â”œâ”€â”€ DSCP
â”‚   â”‚   â”œâ”€â”€ GPT
â”‚   â”‚   â”œâ”€â”€ GrammarPruning
â”‚   â”‚   â”œâ”€â”€ ThinkingMode
â”‚   â”‚   â””â”€â”€ utils
â”‚   â””â”€â”€ FoodOrdering
â”‚       â””â”€â”€ Fine-Tuning and Accuracy Notebooks
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```



### â–¶ï¸ Running Experiments

Refer to implementation notebooks and scripts provided in respective dataset directories for quick and intuitive setup.

## ğŸ“– Citation

If this work helps your research, please consider citing our paper:

```bibtex
@article{grammarpruning2025,
  title={Grammar Pruning: Enabling Low-Latency Zero-Shot Task-Oriented Language Models for Edge AI},
  author={Anonymous},
  journal={Anonymous},
  year={2025}
}
```

## ğŸ“œ License

Distributed under the MIT License. See the [LICENSE](LICENSE) file for more information.
